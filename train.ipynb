{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01a3964b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CyclicLR\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "from utils.loss import ContentLoss, AdversialLoss\n",
    "from utils.transforms import get_default_transforms, get_no_aug_transform\n",
    "from utils.datasets import get_dataloader\n",
    "from utils.transforms import get_pair_transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from models.discriminator import Discriminator\n",
    "from models.generator import Generator\n",
    "\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9da49f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\admin\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\admin\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Config\n",
    "batch_size = 8\n",
    "image_size = 256\n",
    "learning_rate = 1.5e-4\n",
    "beta1, beta2 = (.5, .99)\n",
    "weight_decay = 1e-4\n",
    "epochs = 100\n",
    "\n",
    "# Models\n",
    "netD = Discriminator().to(device)\n",
    "netG = Generator().to(device)\n",
    "\n",
    "# netG = torch.load(\"./checkpoints/pretrained_netG.pth\")\n",
    "netG.load_state_dict(torch.load(\"./checkpoints/init_netG.pth\"))\n",
    "\n",
    "# 优化器配置\n",
    "optimizerD = AdamW(netD.parameters(), lr=learning_rate, betas=(beta1, beta2), weight_decay=weight_decay)\n",
    "optimizerG = AdamW(netG.parameters(), lr=learning_rate, betas=(beta1, beta2), weight_decay=weight_decay)\n",
    "\n",
    "schedulerD = CyclicLR(optimizer=optimizerD, base_lr=learning_rate, max_lr=learning_rate*1e1, cycle_momentum=False)\n",
    "schedulerG = CyclicLR(optimizer=optimizerG, base_lr=learning_rate, max_lr=learning_rate*1e1, cycle_momentum=False)\n",
    "\n",
    "# Labels\n",
    "cartoon_labels = torch.ones (batch_size, 1, image_size // 4, image_size // 4).to(device)\n",
    "fake_labels    = torch.zeros(batch_size, 1, image_size // 4, image_size // 4).to(device)\n",
    "\n",
    "# Loss functions 损失函数\n",
    "content_loss = ContentLoss(omega = 0.1,device = device)\n",
    "adv_loss     = AdversialLoss(cartoon_labels, fake_labels)\n",
    "BCE_loss     = nn.BCELoss().to(device)\n",
    "#内容损失、对抗损失和交叉熵损失\n",
    "\n",
    "# Dataloaders\n",
    "torch.manual_seed(1)\n",
    "real_dataloader    = get_dataloader(\"./datasets/real_images/flickr_nuneaton\", size = image_size, bs = batch_size)\n",
    "cartoon_dataloader = get_dataloader(\"./datasets/cartoon_images_smoothed/Studio Ghibli\", size = image_size, bs = batch_size, trfs=get_pair_transforms(image_size))\n",
    "\n",
    "\n",
    "tracked_images = next(iter(real_dataloader)).to(device)\n",
    "last_epoch = 0\n",
    "last_i = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b29e99",
   "metadata": {},
   "source": [
    "Save original tracked images for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc21e96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_images = tracked_images.detach().cpu()\n",
    "grid = vutils.make_grid(original_images, padding=2, normalize=True, nrow=3)\n",
    "plt.imsave(f\"./results/original.png\", np.transpose(grid, (1,2,0)).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4427cb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# netG.load_state_dict(torch.load(\"./checkpoints/_trained_netG.pth\"))\n",
    "# netD.load_state_dict(torch.load(\"./checkpoints/_trained_netD.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa65669",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #训练状态回复\n",
    "# with open(\"./checkpoints/iter_data.pickle\", \"rb\") as handle:\n",
    "#     last_epoch, last_i = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a73520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training Loop...\n",
      "Adv loss: 0.6826, Content loss: 0.0460\n",
      "[0/100][0/62]            Loss_D: 2.2740           Loss_G: 0.7286           D(x): 0.5503             D(G(z)): 0.6237          2025-04-27_22-14-21\n",
      "Adv loss: 0.6762, Content loss: 0.0373\n",
      "Adv loss: 0.6686, Content loss: 0.0344\n",
      "Adv loss: 0.6634, Content loss: 0.0380\n",
      "Adv loss: 0.6548, Content loss: 0.0404\n",
      "Adv loss: 0.6446, Content loss: 0.0426\n",
      "Adv loss: 0.6327, Content loss: 0.0441\n",
      "Adv loss: 0.6219, Content loss: 0.0398\n",
      "Adv loss: 0.5982, Content loss: 0.0369\n",
      "Adv loss: 0.5732, Content loss: 0.0386\n",
      "Adv loss: 0.5076, Content loss: 0.0523\n",
      "Adv loss: 0.4870, Content loss: 0.0548\n",
      "Adv loss: 0.4510, Content loss: 0.0446\n",
      "Adv loss: 0.4166, Content loss: 0.0588\n",
      "Adv loss: 0.3827, Content loss: 0.0594\n",
      "Adv loss: 0.3449, Content loss: 0.0584\n",
      "Adv loss: 0.3195, Content loss: 0.0607\n",
      "Adv loss: 0.2844, Content loss: 0.0650\n",
      "Adv loss: 0.2732, Content loss: 0.0596\n",
      "Adv loss: 0.2517, Content loss: 0.0574\n",
      "Adv loss: 0.2192, Content loss: 0.0656\n",
      "[0/100][20/62]           Loss_D: 2.3142           Loss_G: 0.2847           D(x): 0.5237             D(G(z)): 0.6927          2025-04-27_22-18-52\n",
      "Adv loss: 0.2367, Content loss: 0.0616\n",
      "Adv loss: 0.2262, Content loss: 0.0837\n",
      "Adv loss: 0.1989, Content loss: 0.0654\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 46\u001b[0m\n\u001b[0;32m     43\u001b[0m errD \u001b[38;5;241m=\u001b[39m adv_loss(cartoon_pred, generated_pred, edge_pred)\n\u001b[0;32m     44\u001b[0m errD\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m---> 46\u001b[0m D_x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msigmoid(cartoon_pred)\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;66;03m# Should be close to 1\u001b[39;00m\n\u001b[0;32m     48\u001b[0m optimizerD\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# （二）训练G\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Lists to keep track of progress\n",
    "img_list = []\n",
    "G_losses = []\n",
    "D_losses = []\n",
    "\n",
    "start_epoch = last_epoch\n",
    "start_i = last_i\n",
    "\n",
    "print(\"Starting Training Loop...\")\n",
    "# 每个epoch\n",
    "for epoch in range(start_epoch, epochs):\n",
    "    # 加载迭代器\n",
    "    real_dl_iter = iter(real_dataloader)\n",
    "    cartoon_dl_iter = iter(cartoon_dataloader)\n",
    "    iterations =  min(len(real_dl_iter), len(cartoon_dl_iter))\n",
    "    \n",
    "    for i in range(start_i, iterations):\n",
    "        real_data = next(real_dl_iter)\n",
    "        cartoon_data = next(cartoon_dl_iter)\n",
    "        \n",
    "        # （一）训练判别器\n",
    "        netD.train()\n",
    "        netG.eval()\n",
    "        \n",
    "        netD.zero_grad()\n",
    "        \n",
    "        edge_data = cartoon_data[:, :, :, :image_size]   \n",
    "        cartoon_data = cartoon_data[:, :, :, image_size:]\n",
    "\n",
    "        # Format batch.\n",
    "        cartoon_data   = cartoon_data.to(device)\n",
    "        edge_data      = edge_data.to(device)\n",
    "        real_data      = real_data.to(device)\n",
    "\n",
    "        # Generate image\n",
    "        generated_data = netG(real_data)\n",
    "        \n",
    "        # Forward pass all batches through D.\n",
    "        cartoon_pred   = netD(cartoon_data)      #.view(-1)\n",
    "        edge_pred      = netD(edge_data)         #.view(-1)\n",
    "        generated_pred = netD(generated_data)    #.view(-1)\n",
    "        \n",
    "        errD = adv_loss(cartoon_pred, generated_pred, edge_pred)\n",
    "        errD.backward()\n",
    "        \n",
    "        D_x = torch.sigmoid(cartoon_pred).mean().item() # Should be close to 1\n",
    "\n",
    "        optimizerD.step()\n",
    "\n",
    "        # （二）训练G\n",
    "        netG.train()\n",
    "        netD.eval()\n",
    "\n",
    "        for param in netD.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        netG.zero_grad()\n",
    "\n",
    "        generated_data = netG(real_data)\n",
    "        generated_pred = netD(generated_data) #.view(-1)\n",
    "\n",
    "        if generated_data.shape[1] == 1:\n",
    "            generated_data = generated_data.repeat(1, 3, 1, 1)\n",
    "        if real_data.shape[1] == 1:\n",
    "            real_data = real_data.repeat(1, 3, 1, 1)\n",
    "\n",
    "        generated_data = generated_data.clamp(0, 1)\n",
    "        real_data = real_data.clamp(0, 1)\n",
    "        cartoon_data = cartoon_data.clamp(0, 1)\n",
    "        edge_data = edge_data.clamp(0, 1)\n",
    "\n",
    "        generated_pred = torch.sigmoid(generated_pred)\n",
    "        \n",
    "        adv = BCE_loss(generated_pred, cartoon_labels)\n",
    "        content = content_loss(generated_data, real_data)\n",
    "        print(f\"Adv loss: {adv.item():.4f}, Content loss: {content.item():.4f}\")\n",
    "        errG = adv + content\n",
    "\n",
    "        errG.backward()\n",
    "\n",
    "        D_G_z2 = torch.sigmoid(generated_pred).mean().item() # Should be close to 1\n",
    "\n",
    "        optimizerG.step()\n",
    "\n",
    "        # Output training stats\n",
    "        if i % 20 == 0:\n",
    "            with torch.no_grad():\n",
    "                fake = netG(tracked_images).detach().cpu()\n",
    "                \n",
    "            grid = vutils.make_grid(fake, padding=2, normalize=True, nrow=3)\n",
    "            time = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "            plt.imsave(f\"./results/E{epoch}_i{i}.png\", np.transpose(grid, (1,2,0)).numpy())\n",
    "            img_list.append(grid)\n",
    "            \n",
    "            print(('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f\\t%s'\n",
    "                % (epoch, epochs, i, iterations, errD.item(), errG.item(), D_x, D_G_z2, time)).expandtabs(25) )\n",
    "            \n",
    "        # Save Losses for plotting later\n",
    "        G_losses.append(errG.item())\n",
    "        D_losses.append(errD.item())\n",
    "        \n",
    "        schedulerD.step()\n",
    "        schedulerG.step()\n",
    "        \n",
    "        last_i = i\n",
    "    \n",
    "    # schedulerD.step()\n",
    "    # schedulerG.step()\n",
    "    start_i = 0\n",
    "    last_epoch = epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3fea0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment if you want to resume prevous traning.\n",
    "torch.save(netG.state_dict(), \"./checkpoints/_trained_netG.pth\")\n",
    "torch.save(netD.state_dict(), \"./checkpoints/_trained_netD.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bded4cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#保存当前训练状态\n",
    "with open(\"./checkpoints/iter_data.pickle\", \"wb\") as handle:\n",
    "    pickle.dump([last_epoch, last_i], handle)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
